## Regression - extra explanations

```{r include=F}
library(tidyverse)
library(webex)
library(cowplot)

studyhabits <- read_csv('https://benwhalley.github.io/rmip/data/studyhabitsandgrades.csv')
first.model <- lm(grade ~ work_hours, data = studyhabits)

```








### The shaded area when using `geom_smooth` {#explanation-shaded-area-geom-smooth}

If you use `geom_smoth` with `method=lm` you get a grey shaded area around the
line.

```{r}
mtcars %>%
  ggplot(aes(wt, mpg)) +
    geom_point() +
    geom_smooth(method=lm)
```

The shaded area shows the **_standard error_** of the line of best fit. This is
an estimate of how confident we are about the predictions the line makes. This
video explains it quite well: <https://www.youtube.com/watch?v=1oHe1a3JqHw>.
When he uses the greek letter \beta he just means "slope".

If you want to hide it, you can add: `se=FALSE`:

```{r}
mtcars %>%
  ggplot(aes(wt, mpg)) +
    geom_point() +
    geom_smooth(method=lm, se=FALSE)
```



### Checking your first predictions {#explanation-first-predictions}

```{r, echo=F, include=F}
preds <- predict(first.model, newdata = tibble(work_hours=c(5,20,40)))
sprintf("%.1f, %.1f and %.1f", preds[1],preds[2],preds[3])

```

You should get something like:
`r sprintf("%.1f, %.1f and %.1f", preds[1],preds[2],preds[3])`

Don't worry about rounding errors... within 1 point is fine.

We should be most confident about the prediction for 20 hours, because we have
more data in the sample which is close to that value. Our line was estimated
from data which didn't have many people who worked 5 or 40 hours, so we don't really
know about those extremes.



### Why don't we always use real data? {#explain-not-real-data}

Real data is often quite complicated, and it is sometimes easier to simulate
data which illustrates a particular teaching point as clearly as possible. It
also lets us create multiple examples quickly.

It _is_ important to use real data though, and this course includes a mix of
both simulated and real data.



### What is a formula? {#explain-formulae}

In R, **formulas** describe the relationship between variables.  They are used widely, e.g. in ggplot, functions like `t.test`, and especially in model-fitting functions like `lm`.

Formulas for regression will always describe the link between one *outcome* and one or more *predictor* variables.

The outcome goes on the left, and predictors go on the right. They are separated by the tilde symbol: the `~`.  When you read `~` you can say in your head *"is predicted by"*.

You can add multiple variables by separating them with a `+` symbol. So `outcome ~ age + gender` is a model where the outcome is predicted by age and gender. This doesn't add interaction terms.

If you want to include interaction terms (e.g. to let slopes vary for different groups) then you use a `*` symbol instead of a plus. So `outcome ~ age * gender` means the outcome is predicted by age, gender, and the interaction of age and gender.


There is a more technical explanation of all of the formula syntax here: <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html>
