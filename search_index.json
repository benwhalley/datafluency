[
["index.html", "Data Fluency", " Data Fluency Ben Whalley and Chris Berry Image via archive.org "],
["overview.html", "Overview", " Overview From the module aims: The module aims to foster fluency and confidence in the handling, visualisation and communication of quantitative data, alongside skills and techniques for working with larger corpuses of textual and other data. Data visualisation is taught as a foundational technique for exploring and communicating insights from quantitative data. Existing knowledge of linear models is extended with the introduction of the generalised linear model, and a contemporary approach, emphasising prediction and model evaluation is introduced. In a nutshell: we want to give you the skills to analyse your data as indepdent researchers, and to give you confidence in working with data which will stand you in good stead in your future careers. "],
["approach-of-this-course.html", "Approach of this course", " Approach of this course Sometimes, psychology students learn statistics through a “bag of tricks” approach: Workshops might teach how to “do an Anova”, or “how run a multiple regression”. Or you might be given a checklist of things to do when analysing data of a particular type, but all without any bigger picture of what we are trying to achieve when we collect and analyse data. To provide a common thread to our teaching, research methods modules at Plymouth adopt the model for the work of data scientists proposed by Wickham, 2017 (see figure): Wickham’s model of a data science workflow In this module we do cover specific skills, but we hope you will also learn about this general approach to working with data, and integrate it into your own research. "],
["format-of-the-sessions.html", "Format of the sessions", " Format of the sessions We have 16 sessions, which work as follows: We avoid extended lectures. This doesn’t work well with this subject matter. The focus is on learning by doing (this is more like cooking than chemisty). In the first hour of each session we will (often) work together. In the second hour your work will be self-paced, or in pairs or small groups. Activities in the workshops are variable in length, sometimes you will finish early, other times you may be expected to complete the activities outside of class. "],
["the-most-important-thinkg-of-all.html", "The most important thinkg of all", " The most important thinkg of all The most important thinkg of all is to do the practice. These materials give lots of practice exercises. You NEED to work through them all to be able to pass the course effectively. Lab diary/R project archive It’s recommended that you keep a running note of all the work you do in class. This can take for form of a notebook in Word, a blog, or an R script (see first session). Without a running record of what you have/haven’t done it’s much harder for teaching staff to help you. The record also allows us to review your progress and make suggestions/improvements. "],
["list-of-sessions.html", "List of sessions", " List of sessions Don’t worry if all the terms in this list don’t make sense yet … they will soon! BeginneRs 1: Getting started with RStudio BeginneRs 2: Basic skills in R Visualisation and plotting: Relationships, group differences, color. Data handing 1: Filtering, selecting and summarising. Data handling 2: Working with multiple files; Rmarkdown. Regression 1: Core concepts. Regression 2: Multiple regression. Regression 3: Tests and variable/effect coding. Uncertainty and intervals: Confidence and prediction intervals. (CHRISTMAS BREAK) Fitting curves: Using polynomials. Building models 1: Choosing variables in multiple regression Building models 2: Anova and Bayes Factors Contrasts and tests: Using emmeans Communicating regression and Anova Reproducing an analysis 1: groups work on example, start to finish. Reproducing an analysis 2: finding a suitable paper, data handling. Reproducing an analysis 3: reproducing the analysis. A catchup/revision session then follows, but no new material is introduced. "],
["access-to-r.html", "Access to R", " Access to R Throughout the module we use R for data processing and analysis. If you are taking this course at Plymouth University, the easiest way to run the code examples here is to the school’s RStudio Server. Login to your account on the server here To get an account on the server, or reset a password, contact the Psychology technical office Installing at home If you want to install R on your own machine, instructions are available here: https://github.com/plymouthPsychology/installR/ Be sure to install the recommended packages or the examples given here won’t work. "],
["reasons-to-use-r.html", "Why do we use R?", " Why do we use R? This material copied from Andy Wills’ RminR. This document covers some of the reasons we use R in this course. It’s not “required reading”, but take a look if you’re interested. Introduction R is a piece of software for handling data. It’s the one used on this course, but it’s not the only option available, others include: Excel, Jamovi, JASP, MATLAB,Stata and, perhaps the most talked-about alternative, SPSS. Student experience Students prefer R. In a recent study, undergraduate psychology students at Glasgow University were given a choice between R and SPSS, having experienced both. Two-thirds of the students chose R. Those who chose R did better in the final assessments and showed lower stats anxiety. R is being used to teach Plymouth University undergraduates (and visiting Year 10 students) across a range of different courses. Read more. Employability Data science is a graduate skill in high demand, and using R is a key skill in that market. In contrast, demand for SPSS skills has been declining dramatically for a decade. At SPSS’s current rate of decline, it’ll be gone by the time you graduate. Read more. Free R is free. You don’t need to pay anything to download or use it, and never will. In contrast, once you leave university, SPSS would cost you or your employer around £6000 per person per year. Never out of date Every analysis you can think of is already available in R, thanks to over 12,000 free packages. As new analyses are developed, they become available in R first. In 2013, SPSS realised it couldn’t keep up with R, and admitted defeat. Real Real data analysis is mainly preprocessing – scientists spend around 80% of their analysis time getting the data into a format where they can apply statistical tests. R is fantastically good at preprocessing. Our course focusses on realistic data analysis, making R the perfect tool for the job. Accurate The alternatives to R for real data analysis are either kludgy, error prone and have poor reproducibility (e.g. preprocessing in Excel, followed by statistics in SPSS), or are more niche in the graduate jobs market (e.g. MATLAB). In particular, Excel is famously error prone with, for example, 1 in 5 experiments in genetics having been screwed up by Excel and the case for the UK government’s policy of financial austerity being based on an Excel screwup. Reproducible R’s use of scripts means that, if you have done the analysis completely in R, you already have a full, reproducible record of your analysis path. Anyone with an internet connection can download R, and reproduce your analysis using your script. Making your analyses reproducible is an essential skill in many areas of research. Free as in freedom R is “free as in freedom” because all the source code is available to everyone (it’s “open source”). Some reasons this is important: All software has bugs; making the source code available means it’s more likely that these bugs are found and fixed. In contrast, no one outside of IBM can look at the source code for SPSS, and it’s entirely up to IBM whether they fix, or tell you about, the bugs it has. All software is eventually abandoned by the people who wrote it (if for no other reason than their death). Open source software only dies if no one in the world cares enough about it to maintain it. In contrast, closed-source software (e.g. SPSS) dies as soon as the current owners decide to kill it. Supported by Plymouth University R is already installed on many public machines at the University of Plymouth. Runs inside a browser You can use R without having to install it, e.g. RStudio Plymouth. "],
["teachers-notes.html", "Teachers’ notes", " Teachers’ notes Course features Organization and staffing The module comprises 17 workshop sessions. Students work individually or in pairs within workshop classes of up to 80. For each session there will be a specified teaching assistant (Ph.D student or TARA) supporting students. Each session is broken in half, with more intensive exercises in the first session, followed by supported-self study or followup activities (e.g. finding literature, working on problems). Resources All sessions in 2019/20 will be run in the a computer lab. The course requires one computer (PC, Mac, or Linux) per student, in all sessions. Any student with a laptop should bring it. For activities based around R, students will use the school RStudio server. Accounts will be released in the first session. There is a need for an online lab book system. Students can use the Blog system within the DLE to keep a record of their activities and progress each week. Employability skills Data handling and visualisation. Communication skills. Working to deadline. Critical thinking / analysis. Pre-module preparation TODO CHECK THIS "],
["week-by-week-resources.html", "Week-by-week resources", " Week-by-week resources TODO ADD WEEK BY WEEK NOTES "],
["assessments.html", "Assessments", " Assessments The assessment for PSYC753 includes 3 components: A pass fail assessment which will ensure you have acquired the core skills in handling and visualising data. A structured data analysis and visualisation task, requiring short answers (50%) An authentic analysis assignment, where you will replicate the analysis of a published study (50%). For submission dates please check the DLE. Don’t rely on this website. "],
["core-skills-test.html", "Core skills test", " Core skills test Details of the core skills test will be released in week 13 via the DLE, and will be due around week 20 (early December). "],
["analysis-and-visualisation-task.html", "Analysis and visualisation task", " Analysis and visualisation task Details of this task will be released in week 25 via the DLE (early January) and coursework will be due around week 31 (mid February). "],
["authentic-analysis-assessment.html", "Authentic analysis assessment", " Authentic analysis assessment This task will be due around week 40. Find an empirical paper for which the authors have freely-shared the dataset but not their analysis scripts. Ideally this will link to your research interests, or be similar to your project analysis, but this is not required. A good way of finding a suitable paper would be to look through a journal known for publishing papers with open-source datasets, for example PlosOne. Another would be to search Zenodo or a similar repository for recently-published datasets, and look for where the articles reporting thse data were published. To keep the assignment simple, choose a paper which uses statistical methods taught as part of the module, or which can be approximated by them. For example: a multiple regression, between-subjects Anova, or repeated measures Anova or mixed model. Some papers report many studies, or many different analyses, and may have technical details with which you are not familiar. If this is the case then you do NOT need to deal with the whole paper: Simply focus on one of the primary analyses. For example, if one of the main study hypotheses can be answered by a 2x2 Anova or mixed model, but there are lots of other analyses included in the paper, then focus on just this one analysis. If you are unsure about the scope of this assignment (e.g. don’t know how much of a paper to try and replicate) then please check with the module leader early in the process. Your task is to: Imagine you were the author of the paper, just prior to collecting data for this study. Complete the form on ‘aspredicted.org’. You don’t actually need to use the aspredicted website for this — simply copy the heading structure from the form, and include your answers in your submitted manuscript. Comment on whether it is possible and/or straightforward to reconstruct what would have been in the pre-registration from what is written in the journal article. Using ggplot, make at least one plot of the data which illustrate the main findings. Explain your design decisions for these plots: what features of the data were you trying to highlight, and how? Replicate the statistical analysis using methods which are appropriate to the data. You do not need to use the exact method used in the original paper: For example, it’s fine to use a mixed model in place of a repeated-measures Anova. Report your findings in APA format. Compare/contrast your results with the published manuscript. Document your responses and version of the analysis in an Rmd file with accompanying dataset, suitable for upload to Zenodo or OSF.io. Reflect on the extent to which the concept of ‘reseacher degrees of freedom’ should influence our understanding of your chosen study. Important notes: Don’t assume that a published dataset and manuscript can be easily replicated simply because they have been shared: It may well be the case that it is hard to replicate the findings and this is OK (and interesting!) You will not lose marks simply because you cannot replicate the published findings. You do not need to use the exact method used in the original paper; the point of the exercise is to apply methods we have learned in an appropriate way. Your mark will be based on how appropriate the analysis you propose is, and the range of skill and understanding you demonstrate in answering the questions. Submitting your answers You should submit exactly 5 files: An Rmd file and datafile which when ‘knitted’ produce An HTML or PDF document. As a separate document, upload a copy of the standard CW coversheet (and complete the feedback section). Also include a pdf copy of the study you chose to replicate. Within the Rmd file you should: Label each question clearly Where necessary, include comments explaining what specific lines of code do. Intersperse explanatory text with code chunks (don’t put everything in the same code chunk). Your Rmd file should “Just Work” when the marker opens and runs it, and produce the same output as the knitted HTML or PDF file you submitted (i.e. there won’t be any errors or missing datafiles). If you work on your own computer at home, you should check your Rmd file ‘knits’ correctly on the online Rstudio server. See common problems when trying to knit Rmd files "],
["beginners.html", "BeginneRs", " BeginneRs Photo: 1ksmiles.com In brief A big part of psychology is collecting data about people, visualizing it (graphs etc.), and drawing conclusions. Working with data is a core skill for researchers, and increasingly important in many professions. RStudio, like Excel, is computer software that helps us to do that. RStudio is rapidly becoming the standard tool for serious data analysis in psychology and other sciences, because it’s powerful, relatively easy to use, and free. In this course we’ll learn R as we go, building a little at a time. This and the next session covers the basics (things you would learn as part of an undergraduate statistics course at Plymouth). If some of this is familiar, there will be extension exercises to enhance your knowledge. This material was adapted from Andy Wills’ RminR. All content on this site distributed under a Creative Commons licence. CC-BY-SA 4.0. "],
["getting-started-rstudio.html", "Getting started with RStudio", " Getting started with RStudio Open a web browser (e.g. Firefox, Safari, Chrome, not Edge) and go to an RStudio server, like the one at: https://rstudio.plymouth.ac.uk. Log on, using the username and password you have been given. If that works, you should see something like this: RStudio on first opening We’ll go through what it all means in a bit. But, first, we’re going to… Create a new project RStudio uses projects to help you keep your work organized, and to make sure you have a record of your analyses. You should start a new project each time you start a new module in your degree (possibly more frequently, but we’ll come back to that later). Here’s how to create a new project: At the top right of RStudio, you will see a little blue cube, with the text “Project: (none)”. Click on this, and select “New project”. RStudio without a project open Now click “New Directory” Project dialog #1 Now click “New Project” Project dialog #2 Next, type in a name for the project that makes sense to you in the “Directory name” box. I’ve typed psyc411, but you should pick something more meaningful to you (e.g. beginning-r). Then click “Create project”. Project dialog #3 Now, create a R script. An R script is a record of the analyses you have done. You create an R Script by clicking on the white plus sign on a green background (see below), and then clicking on “R Script”. Script menu If everything worked well, your screen should now look like this: Project created You should be able to see four parts: The Script window - This is the rectangle on the top left. This is where you will tell R what to do. It only does what you tell it. The Console window - This is the rectangle on the bottom left. This is where R prints the answers to your questions. The Environment window - This is the rectangle on the top right. It’s where R keeps a list of the data it knows about. It’s empty at the moment, because we haven’t given R any data yet. The Files - This is the rectangle on the bottom right. This is a bit like the File Explorer in Windows, or the Finder on a Mac. It shows you what files are in your R project. That’s it! You’re all set to start learning how to analyse data in R. "],
["exploring-data-brief.html", "Exploring data (in brief)", " Exploring data (in brief) Before you start… Before starting this exercise, you should have had a brief introduction to using RStudio. If not, take a look at the Using RStudio worksheet. Contents How to use these worksheets Loading a package Loading data Inspecting data Calculating a mean Dealing with missing data How to use these worksheets. Throughout this worksheet (and others on the course), you’ll see the commands you should type into RStudio inside a grey box, followed by the output you should expect to see in one or more white boxes. Any differences in the colour of the text can be ignored. Each command in this worksheet is followed by one or more explanation sections - those are there to help you understand how the commands work and how to read the output they produce. Loading a package First, we need to load a package called tidyverse. A package is an extension to R that adds new commands. Nearly everything we’ll do in this course uses the tidyverse package, so pretty much every project starts with this instruction. Type (or copy and paste) the command in the grey box into line 1 of the Script window of RStudio. Now, with your cursor still on line 1, press CTRL+ENTER (i.e. press the key marked ‘Ctrl’ and the RETURN or ENTER key together). library(tidyverse) When you do this, line 1 is automatically copied to your Console window and run. Then, RStudio will print some text to the Console (shown in the white box, above). This text tells you that the tidyverse package has loaded (“attached”) some other pacakges (e.g. dplyr). It also tells you that the dplyr package changes the way some commands in R work (“conflicts”). That’s OK. If you get an output that includes the word ‘error’, please see the common problems section. Saving your script You should notice that the name Untitled1 on the Script window has now gone red. This is to remind you that your script has changed since the last time you saved it. So, click on the “Save” icon (the little floppy disk) and save your R script with some kind of meaningful name, for example briefguide.R. The .R indicates that it is an R script. Re-save your script each time you change something in it; that way, you won’t lose any of your work. Loading data Now, we’re going to load some data on the income of 10,000 people in the United States of America. I’ve made up this dataset for teaching purposes, but it’s somewhat similar to large open data sets available on the web, such as US Current Population Survey. Here’s how you get a copy of this data into RStudio so you can start looking at it: Download a copy of the data, by clicking here and saving it to the Downloads folder of your computer. Go to RStudio in your web browser. Click on the ‘Files’ tab in RStudio (bottom right rectangle) Click the ‘Upload’ button. Click ‘Browse…’ Go to your Downloads folder, and select the file you just saved there. Click “OK”. Copy or type the following command into your RStudio script window, and run it (i.e. press CTRL+ENTER while your cursor is on that line) cpsdata &lt;- read_csv(&quot;cps2.csv&quot;) Explanation of the command There are three parts to the command cpsdata &lt;- read_csv(&quot;cps2.csv&quot;): The first part of the command is cpsdata. This gives a name to the data we are going to load. We’ll use this name to refer to it later, so it’s worth using a name that is both short and meaningful. I’ve called it cpsdata because it’s somewhat similar to data from the US Current Population Survey, but you can give data pretty much any name you choose (e.g. fart). The bit in the middle, &lt;-, is an arrow and is typed by pressing &lt; and then -, without a space. This arrow means “put the thing on the right of the arrow into the thing on the left of the arrow”. In Rstudio The last part of the command is read_csv(&quot;cps2.csv&quot;). It loads the data file into cpsdata. The part inside the speech marks, cps2.csv, is the name of the file you just uploaded to your RStudio project. This command can also download data directly from the web, for example read_csv(&quot;http://www.willslab.org.uk/cps2.csv&quot;). This would have been a quicker way to do it in this case, but of course not all data is on a web page. Explanation of the output R likes to print things in red sometimes – this does not mean there’s a problem. If there’s a problem, it will actually say ‘error’. The output here tells us that R has loaded the data, which has eight parts (columns, or cols). It gives us the name of the columns (ID, sex, ...) and tells us what sort of data each column contains: character means the data is words (e.g. ‘female’), double means the data is a number (e.g. ‘42.78’). If you get an error here, please see common errors. Inspecting data Make sure you have just completed the loading data worksheet and have the cps2 dataset loaded. Next, we’ll take a peek at these data. You can do this by clicking on the data in the Environment tab of RStudio, see Using RStudio. We can now see the data set (also known as a data frame). We can see that this data frame has 8 columns and 10000 rows. Each row is one person, and each column provides some information about them. Below is a description of each of the columns. Where you see NA this means this piece of data is missing for this person – quite common in some real datasets. Here’s what each of the columns in the data set contains: Column Description Values ID Unique anonymous participant number 1-10,000 sex Biological sex of participant male, female native Participant born in the US? foreign, native blind Participant blind? yes, no hours Number of hours worked per week a number job Type of job held by participant: charity, nopay, private, public income Annual income in dollars a number education Highest qualification obtained grade-school, high-school, bachelor, master, doctor Calculating a mean Make sure you have recently (in this session) completed the loading data worksheet and have the cps2 dataset loaded. Now we have these data, one question we can ask is “what is the average income of people in the U.S.?” (or, at least, in this sample). In this first example, we’re going to calculate the mean income. As you know, you calculate a mean by adding up all the incomes and dividing by the number of incomes. Our sample has 10,000 participants, so this would be a long and tedious calculation – and we’d probably make an error. It would also be a little bit tedious and error prone in a spreadsheet application (e.g. Excel, Libreoffice Calc). There are some very famous cases of these kinds of “Excel errors” in research, e.g. genetics, economics. In R, we can calculate the mean instantly, and it’s harder to make the sorts of errors that are common in Excel-based analysis. To calculate mean income in R, we add the following command to our script (line 3), and press CTRL+ENTER: cpsdata %&gt;% summarise(mean(income)) # A tibble: 1 x 1 `mean(income)` &lt;dbl&gt; 1 87293. Your output will tell you the mean income in this sample – it’s the last number on the bottom right, and it’s approximately $87,000. If you’re happy with the output you’ve got, move on to the next section. If you would like a more detailed explanation of this output, see more on tibbles. We’ll cover this later anyway. If you get an error here, please see common errors. Explanation of the command This command has three components: The bit on the left, cpsdata, is our data frame, which we loaded and named earlier. The bit in the middle, %&gt;%, is called a pipe. Its job is to send data from one part of your command to another. It is typed by pressing % then &gt; then %, without spaces. So cpsdata %&gt;% sends our data frame to the next part of our command. The bit on the right, summarise(mean(income)) is itself made up of parts. The command summarise does as the name might suggest: it summarises a set of data (cpsdata in this case) into a single number, e.g. a mean. The mean command indicates that the type of summary we want is a mean (there are other summaries, as we will cover later). Finally, income is the name of the column of cpsdata we want to take the mean of – in this case, the income of each individual. Make sure you are 100% clear about the difference between &lt;- and %&gt;%. If you’re not, ask for an explanation in class now. The main clue is to look at the direction of the arrows: %&gt;% sends data from left to right. We call this ‘piping’. &lt;- sends results from the right hand side, to a variable named on the left. This is called assignment. Watch out that -&gt; is not the same as %&gt;%. The thin arrow is always for assignment. You won’t see if often, because it’s normally considered bad manners to use thin right arrows like this (they get confusing). Dealing with missing data To calculate the mean number of hours worked per week, we have to deal with the fact that there is some missing data - we don’t know for all 10,000 people how many hours they work in a week, because they didn’t all tell us. To get a mean of those who did tell us, we tell R to ignore the missing data, like this: cpsdata %&gt;% summarise(mean(hours, na.rm = TRUE)) # A tibble: 1 x 1 `mean(hours, na.rm = TRUE)` &lt;dbl&gt; 1 38.9 Explanation of the command rm is short for ‘remove’, but ‘ignore’ would be a more accurate description, as this command doesn’t delete the NA entries in cpsdata, it just ignores them. So na.rm = TRUE means “ignore the missing data”. If you get an error here, please see common errors. Patterns of missing data Sometime we won’t only want to ignore missing data. We might also want to count how how many variables are missing. The mice package has a useful command for doing this. First, we need to load mice like the did tidyverse above. Type (or copy and paste) the command below into your R script and run it: library(mice) Then we can use the md.pattern() function to describe the patterns of missing data: cpsdata %&gt;% mice::md.pattern() ID sex native income education blind job hours 4483 1 1 1 1 1 1 1 1 0 515 1 1 1 1 1 1 1 0 1 3102 1 1 1 1 1 1 0 0 2 1900 1 1 1 1 1 0 0 0 3 0 0 0 0 0 1900 5002 5517 12419 Explanation of the output md.pattern() produces two outputs: a plot, and a table. In the plot we see The variables in the dataset listed along the top Squares indicating whether a variable is recorded (blue) or missing (purple/red) Each row in the plot is a missing data pattern. So: In the first row, the pattern is that all variables are recorded The pattern in the second row is that all variables bar hours were recorded The third pattern is that job and hours were missing, and so on. The numbers on the left of the plot indicate how many people fit the pattern. So: 4483 people had complete data (pattern 1) 515 people had complete data except for the hours variable (pattern 2) 3102 people were missing hours and job (pattern 3, and so on This can be really helpful when checking whether data has been imported properly, or properly reporting missing data from our experiments (see MacPherson et al. 2010 for current guidelines for clinical trials, which would also be good practice for experimental research). The numbers along the bottom of the plot show how many missing observations there were for the variable marked at the top. So: There were 5517 missing observations for hours, across all participants 5002 for job, and so on. The table provides the same information as the plot, but is perhaps harder to read. References "],
["group-differences-briefly.html", "Group differences (briefly)", " Group differences (briefly) Before you start… Before starting this worksheet, you should have had a brief introduction to using RStudio – Using RStudio. You should also have also completed the worksheet Exploring Data. If not, take a look these earlier worksheets before continuing. If you have completed those worksheets, then you’ll have set up an R project, and you’ll have a script in it that looks something like this: library(tidyverse) library(mice) cpsdata &lt;- read_csv(&quot;cps2.csv&quot;) cpsdata %&gt;% summarise(mean(income)) cpsdata %&gt;% summarise(mean(hours, na.rm = TRUE)) In this worksheet, we’ll add some more commands to this script. Contents Grouping data Drawing a density plot Filtering data Exercise Grouping data One of the most widely discussed issues concerning income is the difference between what men and women, on average, get paid. Let’s have a look at that difference in our teaching sample of 10,000 US participants. In order to do this, we need to split our data into two groups – males and females. In R, the command group_by allows us to do this. In this case, we want to group the data by biological sex, so the command is group_by(sex). We pipe (%&gt;%) the data in cpsdata to the group_by command in order to group it, and then we pipe (%&gt;%) it to summarise to get a summary for each group (a mean, in this case). So, the full command is: cpsdata %&gt;% group_by(sex) %&gt;% summarise(mean(income)) # A tibble: 2 x 2 sex `mean(income)` &lt;chr&gt; &lt;dbl&gt; 1 female 82677. 2 male 92137. Copy it into your script and run it (CTRL+ENTER). Women in our made-up sample get paid, on average, around 9,000 (9k) less than men. Of course, not every male gets 92k a year in the US, and not every female gets 83k. It seems very likely that the range of incomes earned by men and women overlap – meaning that if you picked one man and one woman at random, there’s a reasonable chance that the woman earns more than the man. We can look at this variation in pay using a graph. Looking at variation using a density plot The graph we’re going to draw is a density plot. If you recall histograms from school, it’s a lot like that. If not, don’t worry. A density plot is a curve that shows how likely a range of incomes are. So, the higher the curve is at a particular income, the more people who have that income. We’re going to produce what’s called a scaled density plot. The highest point on a scaled density plot is always one. This can make it easier to compare two groups, particularly if one group has fewer people in it than the other. So here’s the command to do a scaled density plot for incomes, plotting men and women separately. Copy it into your script and run it (CTRL+ENTER). cpsdata %&gt;% ggplot(aes(income, colour=sex)) + geom_density(aes(y=..scaled..)) Explanation of command Here’s what each part of this command means: cpsdata - The data frame containing the data. You created this in the last worksheet. %&gt;% - A pipe. As in the last worksheet, this pipe carries the data in cpsdata to the next part of the command, which does something with it. ggplot() - This means ‘draw me a graph’. All graphs we use in these worksheets use the Grammar for Graphics (gg) plotting commands, so they’ll all include the command ggplot. aes() - Short for aesthetics (what things look like). It means ‘This is the sort of graph I want’. income - I want a graph of the data in the income column of cpsdata color=sex - I want you to give me two graphs on top of each other, in different colours. One colour for men, a different color for women. Use the sex column of cpsdata to work out who is male and who is female. geom_density() - I want this graph to be a density plot. aes(y=..scaled..) - I want this density plot to be scaled (see above). Discussion of output Your graph will appear in the bottom-right window, and should look like the one above. You’ll notice that the two lines seem basically on top of each other … but they can’t be because we know the two groups differ in mean income by over nine thousand dollars! We have a problem to solve… Dealing with extreme data points The problem is one of scale – there are a small number of people who earn very high salaries. In fact, both the highest-paid man, and the highest-paid woman in our sample earn considerably more than one million dollars a year. Filtering data Somehow, we need to deal with the fact that a few people in our sample are very well paid, which makes the difference between men and women hard to see on our graph, despite the difference being over nine thousand dollars a year. One of the easiest ways around this is to exclude these very high salaries from our graph. The vast majority of people are paid less than 150k a year. So, let’s restrict our plotting to just those people. We do this using the filter command. It’s called filter because it works a bit like the filter paper in a chemistry lab (or in your coffee machine) – stopping some things, while letting other things pass through. We can filter our data by telling R what data we want to keep. Here, we want to keep all people who earn less than £150k, and filter out the rest. So the filter we need is filter(income &lt; 150000), where &lt; means “less than”. We’ll be using this dataset of people with &lt;$150k incomes a few times, so we’re going to give it a new name, cpslow (or any other name you want, e.g. angelface ) So, what we need to do is pipe (%&gt;%) our cpsdata data to our filter(income &lt; 150000), and use an arrow, &lt;-, to send this data to our new data frame, cpslow. Recall that &lt;- sends the thing on its right to the thing on its left, so the full command is: cpslow &lt;- cpsdata %&gt;% filter(income &lt; 150000) We can take a look at this new data frame by clicking on it in RStudio’s Environment window. By looking at the ID numbers, you can see that some people in our original sample have been taken out, because they earned at least 150k. Now, we can plot these filtered data in the same way as before, by changing the name of the dataframe from cpsdata to cpslow. So start with the command cpsdata %&gt;% ggplot(aes(income, colour=sex)) + geom_density(aes(y=..scaled..)), copy it onto the next line in your script, make that change, and press CTRL+RETURN. If you’ve got it right, your graph will look like this: At first glance, the two distributions of incomes still look similar. For example, the modal income – the point where the graph is highest – is at quite a low income, and that income is quite similar for both men and women. However, on closer inspection, you’ll also see that the red line (females) is above the blue line (men) until about 25-50k, and below the blue line from then on. This means that more women than men earn less than 50k, and more men than women earn more than 50k. So, the gender pay gap is visible in this graph. The graph also illustrates that the difference in this sample is small, relative to the range of incomes. This doesn’t mean that the gender pay gap is less (or more) important than income inequality. These kinds of questions of importance are moral, philosophical, and political. Data cannot directly answer these kinds of questions, but they can provide information to inform the debate. As we’ll see later, this type of graph is also crucial to inform our choice of statistical models (like regression or Anova): Without having a sense of what the underlying data look like we can make bad decisions in our later analyses. Exercise In this exercise, you’ll consolidate what you’ve learned so far. The task is to further exmaine this sample of participants who are living in the US, and earning less than $150k (cpslow). Specifically, the question to answer is whether people born in the US earn more. In order to do this, you should calculate the mean income for each group, and produce a density plot with one line for each group. Below are the answers you are aiming for: # A tibble: 2 x 2 native `mean(income)` &lt;chr&gt; &lt;dbl&gt; 1 foreign 51423. 2 native 58905. Previously, we calculated the mean salary of men and women. Why might it be a better idea to calculate the median? Show answer Because the data are strongly skewed, the median may be a better summary of the central tendency (the middle). Adapt the commands above to calculate the median instead. What is the median salary for women: , and for men: . Show answers cpsdata %&gt;% group_by(sex)%&gt;% summarise(med=median(income)) # A tibble: 2 x 2 sex med &lt;chr&gt; &lt;dbl&gt; 1 female 52558. 2 male 61746. Extension exercise If you’ve some spare time and are looking for something a bit more challenging, try Exercise 2 on this slightly more advanced worksheet. "],
["undergraduate-stats-in-r.html", "Undergraduate stats in R", " Undergraduate stats in R All of the statistics you will have learned at undergraduate level can be produced in R. Here we cover simple examples of: A t-test A correlation We’ll run these on a example dataset which is built into R, called mtcars. We can look at this data using the glimpse function (which is loaded with tidyverse, so if you get an error make sure that is loaded too): mtcars %&gt;% glimpse() Observations: 32 Variables: 11 $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2,… $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4,… $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140… $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 18… $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92,… $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.1… $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.… $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,… $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,… $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4,… $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1,… Explanation of the glimpse output glimpse produces a list of all variables in the dataset, tells us what type they are, and lists however many obserervations from the dataset that will fit on a single line. The type of all variables in mtcars is dbl. This is short for ‘double-precision number’; for now, just know that dbl means a number. Other types include : int — short for ‘integer’ variable, so only contains whole numbers ( e.g. a participant id number) chr — short for ‘character variable’, which will contain text (e.g. an email address) fct — short for ‘factor’. i.e. a categorical variable (e.g. MCQ responses) ord — short for ‘ordered’. This is variant of categorical variable where the categories have a particular order (responses like “Wost” &lt; “Better” &lt; “Best” could be stored as an ord) Two sample t-test mtcars contains a variable called mpg, which is the miles per gallon each car will do, and another called am which is encodes whether it was a manual or automatic transmission (0=automatic, 1=manual). We can test if mpg differs between auto and manual cars with t.test: t.test(mpg ~ am, data=mtcars) Welch Two Sample t-test data: mpg by am t = -3.7671, df = 18.332, p-value = 0.001374 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -11.280194 -3.209684 sample estimates: mean in group 0 mean in group 1 17.14737 24.39231 Explanation The command contains three parts: t.test: Says what we want to do mpg ~ am: This is a ‘formula’, which tells t.test which variables to analyse. data=mtcars: Which dataset we want to use for the analysis The formula is split into two parts by the ~ symbol. On the left is our outcome. On the right is the grouping variable, which we hope predicts the outcome. In the output you can see the test statistic, degrees of freedom and p value. The tilde symbol. Pronounced “tilder”. In R, ~ almost always means “is predicted by”. Correlations The mtcars data also contains variables for weight (wt) and power (hp, short for horsepower). We can select just these columns and save them to a smaller dataframe like this: carperformance &lt;- mtcars %&gt;% select(mpg, wt, hp) Explanation of the commends On the far left we have the name of the new variable which we will create: carperformance. We can tell this will be a new variable because the &lt;- symbol is just to the right, pointing at it. To work out what carperformance will contain, we look to the right of the &lt;- There are two parts here, linked with the pipe symbol (%&gt;%) which passes data from one command to the next, from left to right. First we see the mtcars data. Using a pipe we pass this to the select command, which selects the mpg,wt, andhp` columns. Explanation of the result When running the command you won’t see any output — but something has happened behind the scenes: A new object was created called carperformance which contained copies of the columns from mtcars we selected. We can see the first few rows of our new smaller dataframe like this: carperformance %&gt;% head() mpg wt hp Mazda RX4 21.0 2.620 110 Mazda RX4 Wag 21.0 2.875 110 Datsun 710 22.8 2.320 93 Hornet 4 Drive 21.4 3.215 110 Hornet Sportabout 18.7 3.440 175 Valiant 18.1 3.460 105 To correlate the three columns in this dataset, we can use the cor function and round all the results to 2 decimal places: carperformance %&gt;% cor() %&gt;% round(2) mpg wt hp mpg 1.00 -0.87 -0.78 wt -0.87 1.00 0.66 hp -0.78 0.66 1.00 Explain those commands… On the left we have the carperformance data. We pipe this to the cor function which calculates the correlation between each pair of columns and returns a special kind of table, called a matrix. To make the output simpler, we then pass the results to the round function, which rounds all the results to 2 decimal places. The cor function is pretty bare-bones, and doesn’t produce output we could easily use in a report or article. The apaTables package helps us with this: apaTables::apa.cor.table(carperformance, filename = &quot;correlations.doc&quot;) Means, standard deviations, and correlations with confidence intervals Variable M SD 1 2 1. mpg 20.09 6.03 2. wt 3.22 0.98 -.87** [-.93, -.74] 3. hp 146.69 68.56 -.78** .66** [-.89, -.59] [.40, .82] Note. M and SD are used to represent mean and standard deviation, respectively. Values in square brackets indicate the 95% confidence interval. The confidence interval is a plausible range of population correlations that could have caused the sample correlation (Cumming, 2014). * indicates p &lt; .05. ** indicates p &lt; .01. Explain the double colons, ::, in the code above… Sometimes we load a whole package, as we did when we wrote library(tidyverse) above. This is a good idea when we want to use lots of functions from that package. When we only want to use one function from a package we can type nameofpackage::nameoffunction and this lets us use the function without loading the package. This can be a good idea if the package or function is less well known, and you want to be explicit about which package it comes from. This can help future-you work out what your code is doing. Explanation of the result We used the apa.cor.table function within the apaTables package to create a nicely-formatted correlation table, in APA format. We also specified a filename, and apa.cor.table created a Word document with this name containing the formatted table (click to see the result). Use one of the other built-in datasets to: Run a correlation between 2 variables. Compute a t-test, using appropriate variables from the data. Describe the results of the t-test in APA format. "],
["visualisation.html", "Visualisation and plotting", " Visualisation and plotting Nothing new under the sun: Mediaeval line plot, circa 1010. Image: wikimedia In brief Visualising data is a core skill for all quantitative researchers, and is one of the most transferable skills taught on the course. Although sometimes underplayed, visualisation plays a much more important role in good science than, for example, statistical testing. Effective visualisations help scientists understand their data, spot errors, and build appropriate models. "],
["session-1.html", "Session 1", " Session 1 We will use R to plot linear relationships and group differences, and to include more than 2 dimensions in a single plot using colour or shape. Along the way we learn additional techniques to transform data to enhance our plots. "],
["data-handling.html", "Data handling", " Data handling In brief Most time in data analysis is spent ‘tidying up’ data: getting it into a suitable format to get started. Data scientists have a particular definition of tidy: Tidy datasets are “easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row” (Wickham 2014). It’s often not convenient for humans to enter data in a tidy way, so untidy data is probably more common than tidy data in the wild. But doing good, reproducible science demands that we document each step of our processing in a way that others can check or repeat in future. Tools like R make this easier. References "],
["sesssion-1.html", "Sesssion 1", " Sesssion 1 We will use the dplyr and data.table libraries to manipulate and reorganise data to solve common problems in experimental and applied research. "],
["session-2.html", "Session 2", " Session 2 We will use techniques to automate the processing of experimental data. We also introduce Rmarkdown to communicate and share our analyses with others, and cover best-practices for data handling and archive. "],
["regression.html", "Regression", " Regression In brief Regression is just a fancy term for drawing the ‘best-fitting’ line through a scatter-plot, and summarising how well the line describes the data. When doing regression in R, the relationship between an outcome and one or more predictors is described using a formula. When a model is ‘fitted’ to a sample it becomes tool to make predictions for future samples. It also allows us to quantify our uncertainty about those predictions. Coefficients are numbers telling us how strong the relationshsips between predictors and outcomes are. But the meaning of coefficients depend on the study design, and the assumptions were are prepared to make. Causal diagrams can help us choose models and interpret our results. Multiple regression is a technique which can describe the relationship between one outcome and two or more predictors. We can also use multiple regression to describe cases where two variable interact. That is, when the effect of one predictor is increased or decreased by another. Multiple regression is important because it allows us to make more realistic models and better predictions. Like any sharp tool, regression should be used carefully. If our statistical model doesn’t match the underlying network of causes and effects, or if we have used a biased sample, we can be misled. "],
["session-1-1.html", "Session 1", " Session 1 We will revise core concepts for selecting and interpreting linear models. Some of the material may have been covered in undergraduate courses, but we will emphasise understanding and mastery of key ideas before we progress further. "],
["session-2-1.html", "Session 2", " Session 2 We extend to examples requiring multiple regression, and interpretation of interaction coefficients. We also cover prediction from models. "],
["session-3.html", "Session 3", " Session 3 We cover statistical testing for individual coefficients and for interactions using frequentist methods (regular p values). "],
["uncertainty.html", "Uncertainty", " Uncertainty Statistical require us to be explicit about our assumptions and help us make numerical predictions for new data. They also let us quantify our uncertainty about these predictions. Quantifying uncertainty is important for both practical and theoretical purposes. Practically, we might want to put ‘bounds’ on our predictions, allowing policy-makers or individuals to make better decisions. Theoretically, we can use estimates of uncertainty to attach probabilities to our experimental hypotheses and this forms part of the process of evaluating existing and new theory. "],
["session-1-2.html", "Session 1", " Session 1 In this session we introduce the concept of intervals (confidence and prediction) and also Bayesian equivalents. "],
["building-models.html", "Building models", " Building models In brief Models need to be appropriately complex. That is, we want to make models that represent our theories for the underlying causes of our data. Often this means adding many variables to a regression model or Anova. But we won’t always be sure which variables to add. Adding multiple variables also brings challenges. Where predictors are correlated (termed multicollinearity then model results can be confusing. Where we would like to choose between alternative models then Bayes Factors can be useful. "],
["session-1-multiple-regression.html", "Session 1: Multiple regression", " Session 1: Multiple regression In this section we cover variable selection in multiple regression, and issues in fitting linear models, including multicollinearity. "],
["session-2-anova-and-bayes-factor.html", "Session 2: Anova and Bayes Factor", " Session 2: Anova and Bayes Factor In this session we discuss model selection in the context of Anova and the use of Bayes Factors to choose between theoretically interesting models. "],
["testing.html", "Testing", " Testing In brief A single statistical model can test many different hypotheses. Sometimes these hypotheses can, superficially, sound similar—but selecting the relevant test and reporting it correctly can sometimes be a challange. R packages exist to make specifying and reporting tests easier, but none can automate the process: Testing hypotheses always requires thought about both the research question in hand alongside statistical issues. The replication crisis has brought renewed focus on the pitfalls of multiple testing and researcher degrees of freedom. Both technical strategies and research policies can mitigate this risk to some degree, but research integrity is crucial. "],
["session-1-planned-contrasts-with-emmeans.html", "Session 1: Planned contrasts with emmeans", " Session 1: Planned contrasts with emmeans In this session we primarily cover the emmeans library and ways to test planned contrasts in an experimental design. "],
["communicating-results.html", "Communicating results", " Communicating results In brief Communicating research findings is an art and a science, but professional standards and integrity demands that we are open, honest and sufficiently skeptical in our reporting of our own data. In academic publishing there are a number of common patterns which can helpful in structuring results sections, breaking-down a complex set of findings and easing interpretation. "],
["session-1-communicating-anova-and-regression.html", "Session 1: Communicating Anova and Regression", " Session 1: Communicating Anova and Regression In this session we practice analysing and writing up real-world examples, compare our work with published findings, and reflect on how our communication might be improved. "],
["real-world-data.html", "Real world data", " Real world data In brief In the real world datasets are never neat and tidy, and findings rarely cut-and-dried. Dealing with complexity and ambiguity is part of the role of the scientist. In these final sessions and the assessment we apply the skills we have learnt to real world examples. "],
["session-1-reproducing-a-real-paper.html", "Session 1: Reproducing a real paper", " Session 1: Reproducing a real paper In this session we dissect a specific published paper, attempt to replicate the authors findings, and reflect on the challenges of reproducing findings given the current state of scientific publishing. "],
["session-2-reproducing-a-real-paper.html", "Session 2: Reproducing a real paper", " Session 2: Reproducing a real paper In this session we work in groups to identify a new published paper for which we might replicate the analysis. You will build on skills learnt in previous weeks to wrangle the published data into a suitable format, and decide which analyses to run. "],
["session-3-reproducing-a-real-paper.html", "Session 3: Reproducing a real paper", " Session 3: Reproducing a real paper We will finalise work on your replication, building towards the individual assessment. "],
["extras.html", "Extras", " Extras "],
["common-problems.html", "Common problems", " Common problems Some of this material was adapted from Andy Wills’ RminR. Try these things first… Check your typing - The command has to be exactly as it appears on the worksheet. In particular, check your brackets ( ), and check that you haven’t missed off the speech marks &quot; &quot;. Check you haven’t missed out any commas ,. Check you have typed captial letters as shown, e.g. bayesfactor is not the same as BayesFactor. Check whether your command uses = or ==, these are not the same thing (see below for explanation). Restart R - If the output you get is very different to what is shown in this worksheet, go to “Session” on the RStudio menu, and select “Restart R”. If that doesn’t fix your problem, see below. Errors when loading a package If you’ve installed R on your own machine, and you get a message like: Error in library(tidyverse) : there is no package called ‘tidyverse’ Have you installed this package? You need to install it before you can use it, see the cheat sheet. Errors when loading data If you get a message like: Error in open.connection(con, &quot;rb&quot;) : Could not resolve host: www.willslab.org.uk Check your internet connection (e.g. by using the web browser to look at your Twitter feed). If you have an internet connection, try the command again. Is it = or == ? In R, = means the same as &lt;-. Computers don’t cope well with situations were the same symbol means two different things, so we use == to mean “equal to”. For example filter(education == &quot;master&quot;) in our income data set means keep the people whose education level equal to “master”. When trying to knit Rmd files Check you have the right number of backticks? Have you used the View() function in your code? This only works when inside RStudio, not when knitting documents. Have you loaded your packages at the top of the file? If you try to call functions from tidyverse or other packages without loading them first you will get errors. Sometimes this is not apparent when working in an interactive session, but creates an error when a package is loaded after it is used in the code. "],
["more-on-tibbles.html", "More on Tibbles’", " More on Tibbles’ This material was adapted from Andy Wills’ RminR. Kitten that might be called Tibbles. Possibly. In the Exploring Data worksheet, you got the mean income of your sample like this: cpsdata %&gt;% summarise(mean(income)) # A tibble: 1 x 1 `mean(income)` &lt;dbl&gt; 1 87293. The answer (the mean) is the last number on the bottom right – but what about all that other stuff in the output? What the hell is a tibble? You can safely ignore the extra stuff but, if you’re curious, here’s what it all means… The first line, A tibble: 1 x 1, says your output is a data frame (aka. tibble) with one row and one column … so, a single number. No, I don’t think anyone knows why it’s called a tibble. The second line tells you what summary you calculated – mean(income), the mean of the incomes. The third line, &lt;dbl&gt; tells you that what you calculated is a number (in case you were wondering dbl is short for ‘double precision floating point number’, but that’s probably more detail than you needed…). The fourth line gives you the number you calculated, the mean income of 87293. The 1 at the beginning is a row number, which you can ignore. If you look at the number 87293. really closely in your output, you’ll notice two more things about it: The first two numbers, 87 are underlined, while the others are not. This is just a way of making big numbers easier to read, much like writing 87,293 rather than 87293. This underlining shows up in your R Console window, but not on these worksheets – sorry about that! There’s a decimal point at the end, but no numbers following the decimal point. This lets you know that the answer has been rounded to the nearest whole number. You’ll always get an answer that is correct to at least three significant figures, which is generally enough for reporting your findings. There are ways to get a more precise output if you need it, but we don’t cover those in this class. Here’s another example. In the Group Differences worksheet of our undergraduate course, we calculate the mean income of your sample by sex like this: cpsdata %&gt;% group_by(sex) %&gt;% summarise(median(income)) # A tibble: 2 x 2 sex `median(income)` &lt;chr&gt; &lt;dbl&gt; 1 female 52558. 2 male 61746. This can be read much the same way as the last example. These are the differences: Your tibble is now 2 x 2 because it has two rows (female, male) and two columns (sex, median income). The &lt;chr&gt; in sex column says that column contains letters (male, female) rather than numbers – chr is short for “characters” (another word for letters). This material is distributed under a Creative Commons licence. CC-BY-SA 4.0. Kitten picture by 0x010C - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=42929793 version 0.2 "],
["self-test-quizzes.html", "Self-test Quizzes", " Self-test Quizzes These self-test quiz questions can be used to test your knowledge each week. "],
["causes-quiz.html", "Causes and effects", " Causes and effects TBC "],
["measurement-quiz.html", "Measurement", " Measurement TBC "],
["data-quiz.html", "Data", " Data TBC "],
["regression-quiz.html", "Regression", " Regression TBC "],
["multiple-regression-quiz.html", "Multiple regression quiz", " Multiple regression quiz TBC "],
["references.html", "References", " References "]
]
